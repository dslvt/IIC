2023-04-12,16:21:27 | INFO | Running with a single process. Device cuda:0.
2023-04-12,16:21:27 | INFO | Loaded coca_ViT-L-14 model config.
2023-04-12,16:21:30 | INFO | Loading pretrained coca_ViT-L-14 weights (mscoco_finetuned_CoCa-ViT-L-14-laion2B-s13B-b90k.bin).
2023-04-12,16:21:32 | INFO | Model:
2023-04-12,16:21:32 | INFO | CoCa(
  (text): TextTransformer(
    (token_embedding): Embedding(49408, 768)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-11): 12 x ResidualAttentionBlock(
          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (visual): VisionTransformer(
    (patchnorm_pre_ln): Identity()
    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
    (patch_dropout): Identity()
    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): ModuleList(
        (0-23): 24 x ResidualAttentionBlock(
          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)
          )
          (ls_1): Identity()
          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)
            (gelu): GELU(approximate='none')
            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)
          )
          (ls_2): Identity()
        )
      )
    )
    (attn_pool): AttentionalPooler(
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
      )
      (ln_q): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (ln_k): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (text_decoder): MultimodalTransformer(
    (resblocks): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (cross_attn): ModuleList(
      (0-11): 12 x ResidualAttentionBlock(
        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (attn): MultiheadAttention(
          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)
        )
        (ls_1): Identity()
        (ln_1_kv): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=768, out_features=3072, bias=True)
          (gelu): GELU(approximate='none')
          (c_proj): Linear(in_features=3072, out_features=768, bias=True)
        )
        (ls_2): Identity()
      )
    )
    (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
)
2023-04-12,16:21:32 | INFO | Params:
2023-04-12,16:21:32 | INFO |   accum_freq: 1
2023-04-12,16:21:32 | INFO |   aug_cfg: {}
2023-04-12,16:21:32 | INFO |   batch_size: 6
2023-04-12,16:21:32 | INFO |   beta1: 0.9
2023-04-12,16:21:32 | INFO |   beta2: 0.98
2023-04-12,16:21:32 | INFO |   checkpoint_path: ./logs/2023_04_12-16_21_27-model_coca_ViT-L-14-lr_1e-05-b_6-j_2-p_amp/checkpoints
2023-04-12,16:21:32 | INFO |   coca_caption_loss_weight: 1.0
2023-04-12,16:21:32 | INFO |   coca_contrastive_loss_weight: 0.0
2023-04-12,16:21:32 | INFO |   copy_codebase: False
2023-04-12,16:21:32 | INFO |   csv_caption_key: title
2023-04-12,16:21:32 | INFO |   csv_img_key: filepath
2023-04-12,16:21:32 | INFO |   csv_separator: 	
2023-04-12,16:21:32 | INFO |   dataset_resampled: False
2023-04-12,16:21:32 | INFO |   dataset_type: csv
2023-04-12,16:21:32 | INFO |   ddp_static_graph: False
2023-04-12,16:21:32 | INFO |   debug: False
2023-04-12,16:21:32 | INFO |   delete_previous_checkpoint: False
2023-04-12,16:21:32 | INFO |   device: cuda:0
2023-04-12,16:21:32 | INFO |   dist_backend: nccl
2023-04-12,16:21:32 | INFO |   dist_url: env://
2023-04-12,16:21:32 | INFO |   distill: False
2023-04-12,16:21:32 | INFO |   distill_model: None
2023-04-12,16:21:32 | INFO |   distill_pretrained: None
2023-04-12,16:21:32 | INFO |   distributed: False
2023-04-12,16:21:32 | INFO |   epochs: 3
2023-04-12,16:21:32 | INFO |   epochs_cooldown: None
2023-04-12,16:21:32 | INFO |   eps: 1e-06
2023-04-12,16:21:32 | INFO |   force_custom_text: False
2023-04-12,16:21:32 | INFO |   force_image_size: None
2023-04-12,16:21:32 | INFO |   force_patch_dropout: None
2023-04-12,16:21:32 | INFO |   force_quick_gelu: False
2023-04-12,16:21:32 | INFO |   gather_with_grad: False
2023-04-12,16:21:32 | INFO |   grad_checkpointing: False
2023-04-12,16:21:32 | INFO |   grad_clip_norm: None
2023-04-12,16:21:32 | INFO |   horovod: False
2023-04-12,16:21:32 | INFO |   image_mean: None
2023-04-12,16:21:32 | INFO |   image_std: None
2023-04-12,16:21:32 | INFO |   imagenet_v2: None
2023-04-12,16:21:32 | INFO |   imagenet_val: None
2023-04-12,16:21:32 | INFO |   local_loss: False
2023-04-12,16:21:32 | INFO |   local_rank: 0
2023-04-12,16:21:32 | INFO |   lock_image: False
2023-04-12,16:21:32 | INFO |   lock_image_freeze_bn_stats: False
2023-04-12,16:21:32 | INFO |   lock_image_unlocked_groups: 0
2023-04-12,16:21:32 | INFO |   lock_text: False
2023-04-12,16:21:32 | INFO |   lock_text_freeze_layer_norm: False
2023-04-12,16:21:32 | INFO |   lock_text_unlocked_layers: 0
2023-04-12,16:21:32 | INFO |   log_every_n_steps: 100
2023-04-12,16:21:32 | INFO |   log_level: 20
2023-04-12,16:21:32 | INFO |   log_local: False
2023-04-12,16:21:32 | INFO |   log_path: ./logs/2023_04_12-16_21_27-model_coca_ViT-L-14-lr_1e-05-b_6-j_2-p_amp/out.log
2023-04-12,16:21:32 | INFO |   logs: ./logs/
2023-04-12,16:21:32 | INFO |   lr: 1e-05
2023-04-12,16:21:32 | INFO |   lr_cooldown_end: 0.0
2023-04-12,16:21:32 | INFO |   lr_cooldown_power: 1.0
2023-04-12,16:21:32 | INFO |   lr_scheduler: cosine
2023-04-12,16:21:32 | INFO |   model: coca_ViT-L-14
2023-04-12,16:21:32 | INFO |   name: 2023_04_12-16_21_27-model_coca_ViT-L-14-lr_1e-05-b_6-j_2-p_amp
2023-04-12,16:21:32 | INFO |   no_set_device_rank: False
2023-04-12,16:21:32 | INFO |   precision: amp
2023-04-12,16:21:32 | INFO |   pretrained: mscoco_finetuned_CoCa-ViT-L-14-laion2B-s13B-b90k.bin
2023-04-12,16:21:32 | INFO |   pretrained_image: False
2023-04-12,16:21:32 | INFO |   rank: 0
2023-04-12,16:21:32 | INFO |   remote_sync: None
2023-04-12,16:21:32 | INFO |   remote_sync_frequency: 300
2023-04-12,16:21:32 | INFO |   remote_sync_protocol: s3
2023-04-12,16:21:32 | INFO |   report_to: 
2023-04-12,16:21:32 | INFO |   resume: None
2023-04-12,16:21:32 | INFO |   save_frequency: 1
2023-04-12,16:21:32 | INFO |   save_most_recent: False
2023-04-12,16:21:32 | INFO |   seed: 0
2023-04-12,16:21:32 | INFO |   skip_scheduler: False
2023-04-12,16:21:32 | INFO |   tensorboard: False
2023-04-12,16:21:32 | INFO |   tensorboard_path: 
2023-04-12,16:21:32 | INFO |   torchscript: False
2023-04-12,16:21:32 | INFO |   trace: False
2023-04-12,16:21:32 | INFO |   train_data: train_data.csv
2023-04-12,16:21:32 | INFO |   train_data_upsampling_factors: None
2023-04-12,16:21:32 | INFO |   train_num_samples: None
2023-04-12,16:21:32 | INFO |   use_bn_sync: False
2023-04-12,16:21:32 | INFO |   val_data: None
2023-04-12,16:21:32 | INFO |   val_frequency: 1
2023-04-12,16:21:32 | INFO |   val_num_samples: None
2023-04-12,16:21:32 | INFO |   wandb: False
2023-04-12,16:21:32 | INFO |   wandb_notes: 
2023-04-12,16:21:32 | INFO |   wandb_project_name: open-clip
2023-04-12,16:21:32 | INFO |   warmup: 1000
2023-04-12,16:21:32 | INFO |   wd: 0.1
2023-04-12,16:21:32 | INFO |   workers: 2
2023-04-12,16:21:32 | INFO |   world_size: 1
2023-04-12,16:21:32 | INFO |   zeroshot_frequency: 2
2023-04-12,16:21:32 | INFO | Start epoch 0
2023-04-12,16:21:34 | INFO | Train Epoch: 0 [    6/29999 (0%)] Data (t): 0.160 Batch (t): 1.828, 3.28285/s, 3.28285/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 4.5896 (4.5896) Loss: 4.5896 (4.5896)
2023-04-12,16:21:59 | INFO | Train Epoch: 0 [  606/29999 (2%)] Data (t): 0.000 Batch (t): 0.250, 23.6379/s, 23.6379/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 4.1875 (4.3886) Loss: 4.1875 (4.3886)
2023-04-12,16:22:24 | INFO | Train Epoch: 0 [ 1206/29999 (4%)] Data (t): 0.001 Batch (t): 0.254, 23.6339/s, 23.6339/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 3.2767 (4.0179) Loss: 3.2767 (4.0179)
2023-04-12,16:22:50 | INFO | Train Epoch: 0 [ 1806/29999 (6%)] Data (t): 0.001 Batch (t): 0.255, 23.5568/s, 23.5568/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 3.8865 (3.9851) Loss: 3.8865 (3.9851)
2023-04-12,16:23:15 | INFO | Train Epoch: 0 [ 2406/29999 (8%)] Data (t): 0.001 Batch (t): 0.255, 23.6327/s, 23.6327/s/gpu LR: 0.000004 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.7360 (3.7353) Loss: 2.7360 (3.7353)
2023-04-12,16:23:41 | INFO | Train Epoch: 0 [ 3006/29999 (10%)] Data (t): 0.001 Batch (t): 0.255, 23.5538/s, 23.5538/s/gpu LR: 0.000005 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.0871 (3.4606) Loss: 2.0871 (3.4606)
2023-04-12,16:24:06 | INFO | Train Epoch: 0 [ 3606/29999 (12%)] Data (t): 0.001 Batch (t): 0.255, 23.4933/s, 23.4933/s/gpu LR: 0.000006 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.6409 (3.3435) Loss: 2.6409 (3.3435)
2023-04-12,16:24:32 | INFO | Train Epoch: 0 [ 4206/29999 (14%)] Data (t): 0.001 Batch (t): 0.255, 23.4876/s, 23.4876/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 3.5355 (3.3675) Loss: 3.5355 (3.3675)
2023-04-12,16:24:57 | INFO | Train Epoch: 0 [ 4806/29999 (16%)] Data (t): 0.001 Batch (t): 0.254, 23.5819/s, 23.5819/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 3.0037 (3.3271) Loss: 3.0037 (3.3271)
2023-04-12,16:25:23 | INFO | Train Epoch: 0 [ 5406/29999 (18%)] Data (t): 0.001 Batch (t): 0.256, 23.5000/s, 23.5000/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 3.0833 (3.3027) Loss: 3.0833 (3.3027)
2023-04-12,16:25:48 | INFO | Train Epoch: 0 [ 6006/29999 (20%)] Data (t): 0.001 Batch (t): 0.255, 23.5603/s, 23.5603/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.8362 (3.2603) Loss: 2.8362 (3.2603)
2023-04-12,16:26:14 | INFO | Train Epoch: 0 [ 6606/29999 (22%)] Data (t): 0.001 Batch (t): 0.254, 23.4314/s, 23.4314/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.7541 (3.2181) Loss: 2.7541 (3.2181)
2023-04-12,16:26:39 | INFO | Train Epoch: 0 [ 7206/29999 (24%)] Data (t): 0.001 Batch (t): 0.255, 23.4876/s, 23.4876/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.7846 (3.1847) Loss: 2.7846 (3.1847)
2023-04-12,16:27:05 | INFO | Train Epoch: 0 [ 7806/29999 (26%)] Data (t): 0.001 Batch (t): 0.255, 23.5198/s, 23.5198/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.7550 (3.1540) Loss: 2.7550 (3.1540)
2023-04-12,16:27:30 | INFO | Train Epoch: 0 [ 8406/29999 (28%)] Data (t): 0.001 Batch (t): 0.255, 23.5493/s, 23.5493/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.4364 (3.1062) Loss: 2.4364 (3.1062)
2023-04-12,16:27:56 | INFO | Train Epoch: 0 [ 9006/29999 (30%)] Data (t): 0.001 Batch (t): 0.255, 23.4715/s, 23.4715/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.2140 (3.0504) Loss: 2.2140 (3.0504)
2023-04-12,16:28:21 | INFO | Train Epoch: 0 [ 9606/29999 (32%)] Data (t): 0.001 Batch (t): 0.255, 23.4613/s, 23.4613/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.2580 (3.0038) Loss: 2.2580 (3.0038)
2023-04-12,16:28:47 | INFO | Train Epoch: 0 [10206/29999 (34%)] Data (t): 0.001 Batch (t): 0.256, 23.5765/s, 23.5765/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.0828 (2.9527) Loss: 2.0828 (2.9527)
2023-04-12,16:29:12 | INFO | Train Epoch: 0 [10806/29999 (36%)] Data (t): 0.001 Batch (t): 0.255, 23.5414/s, 23.5414/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.3519 (2.9210) Loss: 2.3519 (2.9210)
2023-04-12,16:29:38 | INFO | Train Epoch: 0 [11406/29999 (38%)] Data (t): 0.001 Batch (t): 0.255, 23.5604/s, 23.5604/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.4486 (2.8474) Loss: 1.4486 (2.8474)
2023-04-12,16:30:03 | INFO | Train Epoch: 0 [12006/29999 (40%)] Data (t): 0.001 Batch (t): 0.255, 23.5589/s, 23.5589/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.0877 (2.8112) Loss: 2.0877 (2.8112)
2023-04-12,16:30:29 | INFO | Train Epoch: 0 [12606/29999 (42%)] Data (t): 0.001 Batch (t): 0.255, 23.4454/s, 23.4454/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.7762 (2.7642) Loss: 1.7762 (2.7642)
2023-04-12,16:30:54 | INFO | Train Epoch: 0 [13206/29999 (44%)] Data (t): 0.001 Batch (t): 0.255, 23.5614/s, 23.5614/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.6212 (2.7145) Loss: 1.6212 (2.7145)
2023-04-12,16:31:20 | INFO | Train Epoch: 0 [13806/29999 (46%)] Data (t): 0.001 Batch (t): 0.255, 23.4800/s, 23.4800/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.1273 (2.6900) Loss: 2.1273 (2.6900)
2023-04-12,16:31:45 | INFO | Train Epoch: 0 [14406/29999 (48%)] Data (t): 0.001 Batch (t): 0.255, 19.5958/s, 19.5958/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.6566 (2.6487) Loss: 1.6566 (2.6487)
2023-04-12,16:32:11 | INFO | Train Epoch: 0 [15006/29999 (50%)] Data (t): 0.001 Batch (t): 0.255, 23.5445/s, 23.5445/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.8620 (2.6184) Loss: 1.8620 (2.6184)
2023-04-12,16:32:36 | INFO | Train Epoch: 0 [15606/29999 (52%)] Data (t): 0.001 Batch (t): 0.255, 23.5496/s, 23.5496/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.7838 (2.5875) Loss: 1.7838 (2.5875)
2023-04-12,16:33:02 | INFO | Train Epoch: 0 [16206/29999 (54%)] Data (t): 0.001 Batch (t): 0.255, 23.5488/s, 23.5488/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.9780 (2.5658) Loss: 1.9780 (2.5658)
2023-04-12,16:33:27 | INFO | Train Epoch: 0 [16806/29999 (56%)] Data (t): 0.001 Batch (t): 0.255, 23.5651/s, 23.5651/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.0379 (2.5476) Loss: 2.0379 (2.5476)
2023-04-12,16:33:53 | INFO | Train Epoch: 0 [17406/29999 (58%)] Data (t): 0.001 Batch (t): 0.255, 23.5271/s, 23.5271/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.0690 (2.5316) Loss: 2.0690 (2.5316)
2023-04-12,16:34:18 | INFO | Train Epoch: 0 [18006/29999 (60%)] Data (t): 0.001 Batch (t): 0.255, 23.5745/s, 23.5745/s/gpu LR: 0.000010 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.6900 (2.5045) Loss: 1.6900 (2.5045)
2023-04-12,16:34:44 | INFO | Train Epoch: 0 [18606/29999 (62%)] Data (t): 0.001 Batch (t): 0.255, 23.5393/s, 23.5393/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 3.0336 (2.5210) Loss: 3.0336 (2.5210)
2023-04-12,16:35:09 | INFO | Train Epoch: 0 [19206/29999 (64%)] Data (t): 0.001 Batch (t): 0.255, 23.5176/s, 23.5176/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.7306 (2.4970) Loss: 1.7306 (2.4970)
2023-04-12,16:35:35 | INFO | Train Epoch: 0 [19806/29999 (66%)] Data (t): 0.001 Batch (t): 0.255, 23.5326/s, 23.5326/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.6825 (2.5025) Loss: 2.6825 (2.5025)
2023-04-12,16:36:00 | INFO | Train Epoch: 0 [20406/29999 (68%)] Data (t): 0.001 Batch (t): 0.255, 23.5095/s, 23.5095/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.7365 (2.4806) Loss: 1.7365 (2.4806)
2023-04-12,16:36:26 | INFO | Train Epoch: 0 [21006/29999 (70%)] Data (t): 0.001 Batch (t): 0.255, 23.5045/s, 23.5045/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.8086 (2.4619) Loss: 1.8086 (2.4619)
2023-04-12,16:36:51 | INFO | Train Epoch: 0 [21606/29999 (72%)] Data (t): 0.001 Batch (t): 0.255, 23.6110/s, 23.6110/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.5263 (2.4367) Loss: 1.5263 (2.4367)
2023-04-12,16:37:17 | INFO | Train Epoch: 0 [22206/29999 (74%)] Data (t): 0.001 Batch (t): 0.255, 23.5581/s, 23.5581/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.0471 (2.4001) Loss: 1.0471 (2.4001)
2023-04-12,16:37:42 | INFO | Train Epoch: 0 [22806/29999 (76%)] Data (t): 0.001 Batch (t): 0.255, 23.4704/s, 23.4704/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.82420 (2.3597) Loss: 0.82420 (2.3597)
2023-04-12,16:38:08 | INFO | Train Epoch: 0 [23406/29999 (78%)] Data (t): 0.001 Batch (t): 0.255, 23.5546/s, 23.5546/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.9510 (2.3495) Loss: 1.9510 (2.3495)
2023-04-12,16:38:33 | INFO | Train Epoch: 0 [24006/29999 (80%)] Data (t): 0.001 Batch (t): 0.255, 23.5462/s, 23.5462/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.0575 (2.3423) Loss: 2.0575 (2.3423)
2023-04-12,16:38:59 | INFO | Train Epoch: 0 [24606/29999 (82%)] Data (t): 0.001 Batch (t): 0.255, 23.4789/s, 23.4789/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.6404 (2.3256) Loss: 1.6404 (2.3256)
2023-04-12,16:39:24 | INFO | Train Epoch: 0 [25206/29999 (84%)] Data (t): 0.001 Batch (t): 0.255, 23.5320/s, 23.5320/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.8845 (2.3154) Loss: 1.8845 (2.3154)
2023-04-12,16:39:50 | INFO | Train Epoch: 0 [25806/29999 (86%)] Data (t): 0.001 Batch (t): 0.255, 23.5589/s, 23.5589/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.6551 (2.3004) Loss: 1.6551 (2.3004)
2023-04-12,16:40:15 | INFO | Train Epoch: 0 [26406/29999 (88%)] Data (t): 0.001 Batch (t): 0.255, 23.5472/s, 23.5472/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.0052 (2.2716) Loss: 1.0052 (2.2716)
2023-04-12,16:40:41 | INFO | Train Epoch: 0 [27006/29999 (90%)] Data (t): 0.001 Batch (t): 0.255, 23.5645/s, 23.5645/s/gpu LR: 0.000009 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.0339 (2.2447) Loss: 1.0339 (2.2447)
2023-04-12,16:41:06 | INFO | Train Epoch: 0 [27606/29999 (92%)] Data (t): 0.001 Batch (t): 0.255, 23.5757/s, 23.5757/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.1553 (2.2428) Loss: 2.1553 (2.2428)
2023-04-12,16:41:32 | INFO | Train Epoch: 0 [28206/29999 (94%)] Data (t): 0.001 Batch (t): 0.255, 23.5582/s, 23.5582/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.2535 (2.2430) Loss: 2.2535 (2.2430)
2023-04-12,16:41:57 | INFO | Train Epoch: 0 [28806/29999 (96%)] Data (t): 0.001 Batch (t): 0.255, 23.5607/s, 23.5607/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.9510 (2.2370) Loss: 1.9510 (2.2370)
2023-04-12,16:42:23 | INFO | Train Epoch: 0 [29406/29999 (98%)] Data (t): 0.001 Batch (t): 0.255, 23.4627/s, 23.4627/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.64983 (2.2053) Loss: 0.64983 (2.2053)
2023-04-12,16:42:48 | INFO | Train Epoch: 0 [29994/29999 (100%)] Data (t): 0.001 Batch (t): 0.255, 23.6164/s, 23.6164/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.3704 (2.2085) Loss: 2.3704 (2.2085)
2023-04-12,16:42:53 | INFO | Start epoch 1
2023-04-12,16:42:54 | INFO | Train Epoch: 1 [    6/29999 (0%)] Data (t): 0.183 Batch (t): 0.392, 15.2898/s, 15.2898/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.1234 (1.1234) Loss: 1.1234 (1.1234)
2023-04-12,16:43:19 | INFO | Train Epoch: 1 [  606/29999 (2%)] Data (t): 0.001 Batch (t): 0.255, 23.6027/s, 23.6027/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.9542 (1.5388) Loss: 1.9542 (1.5388)
2023-04-12,16:43:45 | INFO | Train Epoch: 1 [ 1206/29999 (4%)] Data (t): 0.001 Batch (t): 0.255, 23.4971/s, 23.4971/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.4161 (1.4979) Loss: 1.4161 (1.4979)
2023-04-12,16:44:10 | INFO | Train Epoch: 1 [ 1806/29999 (6%)] Data (t): 0.001 Batch (t): 0.255, 23.4612/s, 23.4612/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.1491 (1.4107) Loss: 1.1491 (1.4107)
2023-04-12,16:44:36 | INFO | Train Epoch: 1 [ 2406/29999 (8%)] Data (t): 0.001 Batch (t): 0.255, 23.5041/s, 23.5041/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.5746 (1.4435) Loss: 1.5746 (1.4435)
2023-04-12,16:45:01 | INFO | Train Epoch: 1 [ 3006/29999 (10%)] Data (t): 0.001 Batch (t): 0.255, 23.6093/s, 23.6093/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.2521 (1.4116) Loss: 1.2521 (1.4116)
2023-04-12,16:45:26 | INFO | Train Epoch: 1 [ 3606/29999 (12%)] Data (t): 0.001 Batch (t): 0.255, 23.6367/s, 23.6367/s/gpu LR: 0.000008 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.85556 (1.3321) Loss: 0.85556 (1.3321)
2023-04-12,16:45:52 | INFO | Train Epoch: 1 [ 4206/29999 (14%)] Data (t): 0.001 Batch (t): 0.255, 23.5724/s, 23.5724/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.75977 (1.2606) Loss: 0.75977 (1.2606)
2023-04-12,16:46:17 | INFO | Train Epoch: 1 [ 4806/29999 (16%)] Data (t): 0.001 Batch (t): 0.255, 23.5896/s, 23.5896/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.63233 (1.1908) Loss: 0.63233 (1.1908)
2023-04-12,16:46:43 | INFO | Train Epoch: 1 [ 5406/29999 (18%)] Data (t): 0.001 Batch (t): 0.256, 23.5668/s, 23.5668/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.2007 (1.1918) Loss: 1.2007 (1.1918)
2023-04-12,16:47:08 | INFO | Train Epoch: 1 [ 6006/29999 (20%)] Data (t): 0.001 Batch (t): 0.254, 23.5914/s, 23.5914/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.7476 (1.2423) Loss: 1.7476 (1.2423)
2023-04-12,16:47:34 | INFO | Train Epoch: 1 [ 6606/29999 (22%)] Data (t): 0.001 Batch (t): 0.255, 23.6007/s, 23.6007/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.0924 (1.2298) Loss: 1.0924 (1.2298)
2023-04-12,16:47:59 | INFO | Train Epoch: 1 [ 7206/29999 (24%)] Data (t): 0.001 Batch (t): 0.255, 23.5049/s, 23.5049/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.1511 (1.2238) Loss: 1.1511 (1.2238)
2023-04-12,16:48:25 | INFO | Train Epoch: 1 [ 7806/29999 (26%)] Data (t): 0.001 Batch (t): 0.254, 23.5518/s, 23.5518/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.94823 (1.2041) Loss: 0.94823 (1.2041)
2023-04-12,16:48:50 | INFO | Train Epoch: 1 [ 8406/29999 (28%)] Data (t): 0.001 Batch (t): 0.255, 23.5058/s, 23.5058/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.81069 (1.1779) Loss: 0.81069 (1.1779)
2023-04-12,16:49:16 | INFO | Train Epoch: 1 [ 9006/29999 (30%)] Data (t): 0.001 Batch (t): 0.255, 23.5574/s, 23.5574/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.0477 (1.2322) Loss: 2.0477 (1.2322)
2023-04-12,16:49:41 | INFO | Train Epoch: 1 [ 9606/29999 (32%)] Data (t): 0.001 Batch (t): 0.255, 23.5763/s, 23.5763/s/gpu LR: 0.000007 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.72736 (1.2025) Loss: 0.72736 (1.2025)
2023-04-12,16:50:07 | INFO | Train Epoch: 1 [10206/29999 (34%)] Data (t): 0.001 Batch (t): 0.255, 23.5719/s, 23.5719/s/gpu LR: 0.000006 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.2668 (1.2616) Loss: 2.2668 (1.2616)
2023-04-12,16:50:32 | INFO | Train Epoch: 1 [10806/29999 (36%)] Data (t): 0.001 Batch (t): 0.255, 23.5882/s, 23.5882/s/gpu LR: 0.000006 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.88830 (1.2420) Loss: 0.88830 (1.2420)
2023-04-12,16:50:58 | INFO | Train Epoch: 1 [11406/29999 (38%)] Data (t): 0.001 Batch (t): 0.255, 23.5615/s, 23.5615/s/gpu LR: 0.000006 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.39639 (1.1997) Loss: 0.39639 (1.1997)
2023-04-12,16:51:23 | INFO | Train Epoch: 1 [12006/29999 (40%)] Data (t): 0.001 Batch (t): 0.255, 23.5202/s, 23.5202/s/gpu LR: 0.000006 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.14789 (1.1496) Loss: 0.14789 (1.1496)
2023-04-12,16:51:49 | INFO | Train Epoch: 1 [12606/29999 (42%)] Data (t): 0.001 Batch (t): 0.255, 23.5832/s, 23.5832/s/gpu LR: 0.000006 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.93056 (1.1397) Loss: 0.93056 (1.1397)
2023-04-12,16:52:14 | INFO | Train Epoch: 1 [13206/29999 (44%)] Data (t): 0.001 Batch (t): 0.254, 23.5091/s, 23.5091/s/gpu LR: 0.000006 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.66030 (1.1188) Loss: 0.66030 (1.1188)
2023-04-12,16:52:40 | INFO | Train Epoch: 1 [13806/29999 (46%)] Data (t): 0.001 Batch (t): 0.255, 23.5860/s, 23.5860/s/gpu LR: 0.000006 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.3981 (1.1305) Loss: 1.3981 (1.1305)
2023-04-12,16:53:05 | INFO | Train Epoch: 1 [14406/29999 (48%)] Data (t): 0.001 Batch (t): 0.255, 23.5582/s, 23.5582/s/gpu LR: 0.000006 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.96900 (1.1240) Loss: 0.96900 (1.1240)
2023-04-12,16:53:31 | INFO | Train Epoch: 1 [15006/29999 (50%)] Data (t): 0.001 Batch (t): 0.255, 23.5563/s, 23.5563/s/gpu LR: 0.000006 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.7319 (1.1474) Loss: 1.7319 (1.1474)
2023-04-12,16:53:56 | INFO | Train Epoch: 1 [15606/29999 (52%)] Data (t): 0.001 Batch (t): 0.255, 23.5624/s, 23.5624/s/gpu LR: 0.000005 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.83819 (1.1359) Loss: 0.83819 (1.1359)
2023-04-12,16:54:21 | INFO | Train Epoch: 1 [16206/29999 (54%)] Data (t): 0.001 Batch (t): 0.254, 23.5588/s, 23.5588/s/gpu LR: 0.000005 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.74059 (1.1218) Loss: 0.74059 (1.1218)
2023-04-12,16:54:47 | INFO | Train Epoch: 1 [16806/29999 (56%)] Data (t): 0.001 Batch (t): 0.255, 23.5019/s, 23.5019/s/gpu LR: 0.000005 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.78862 (1.1103) Loss: 0.78862 (1.1103)
2023-04-12,16:55:12 | INFO | Train Epoch: 1 [17406/29999 (58%)] Data (t): 0.001 Batch (t): 0.255, 23.5011/s, 23.5011/s/gpu LR: 0.000005 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.86230 (1.1021) Loss: 0.86230 (1.1021)
2023-04-12,16:55:38 | INFO | Train Epoch: 1 [18006/29999 (60%)] Data (t): 0.001 Batch (t): 0.255, 23.5827/s, 23.5827/s/gpu LR: 0.000005 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.3210 (1.1091) Loss: 1.3210 (1.1091)
2023-04-12,16:56:03 | INFO | Train Epoch: 1 [18606/29999 (62%)] Data (t): 0.001 Batch (t): 0.255, 23.5417/s, 23.5417/s/gpu LR: 0.000005 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.73010 (1.0973) Loss: 0.73010 (1.0973)
2023-04-12,16:56:29 | INFO | Train Epoch: 1 [19206/29999 (64%)] Data (t): 0.001 Batch (t): 0.255, 23.5838/s, 23.5838/s/gpu LR: 0.000005 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.30964 (1.0734) Loss: 0.30964 (1.0734)
2023-04-12,16:56:54 | INFO | Train Epoch: 1 [19806/29999 (66%)] Data (t): 0.001 Batch (t): 0.255, 23.4996/s, 23.4996/s/gpu LR: 0.000005 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 2.4190 (1.1130) Loss: 2.4190 (1.1130)
2023-04-12,16:57:20 | INFO | Train Epoch: 1 [20406/29999 (68%)] Data (t): 0.001 Batch (t): 0.255, 23.5891/s, 23.5891/s/gpu LR: 0.000005 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.31753 (1.0903) Loss: 0.31753 (1.0903)
2023-04-12,16:57:45 | INFO | Train Epoch: 1 [21006/29999 (70%)] Data (t): 0.001 Batch (t): 0.255, 23.4135/s, 23.4135/s/gpu LR: 0.000004 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.70005 (1.0794) Loss: 0.70005 (1.0794)
2023-04-12,16:58:11 | INFO | Train Epoch: 1 [21606/29999 (72%)] Data (t): 0.001 Batch (t): 0.255, 23.5523/s, 23.5523/s/gpu LR: 0.000004 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.58466 (1.0660) Loss: 0.58466 (1.0660)
2023-04-12,16:58:36 | INFO | Train Epoch: 1 [22206/29999 (74%)] Data (t): 0.001 Batch (t): 0.255, 23.4145/s, 23.4145/s/gpu LR: 0.000004 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.0041 (1.0644) Loss: 1.0041 (1.0644)
2023-04-12,16:59:02 | INFO | Train Epoch: 1 [22806/29999 (76%)] Data (t): 0.001 Batch (t): 0.256, 23.5649/s, 23.5649/s/gpu LR: 0.000004 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.0016 (1.0628) Loss: 1.0016 (1.0628)
2023-04-12,16:59:27 | INFO | Train Epoch: 1 [23406/29999 (78%)] Data (t): 0.001 Batch (t): 0.255, 23.5847/s, 23.5847/s/gpu LR: 0.000004 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.3132 (1.0691) Loss: 1.3132 (1.0691)
2023-04-12,16:59:53 | INFO | Train Epoch: 1 [24006/29999 (80%)] Data (t): 0.001 Batch (t): 0.255, 23.4879/s, 23.4879/s/gpu LR: 0.000004 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.1958 (1.0722) Loss: 1.1958 (1.0722)
2023-04-12,17:00:18 | INFO | Train Epoch: 1 [24606/29999 (82%)] Data (t): 0.001 Batch (t): 0.254, 23.5760/s, 23.5760/s/gpu LR: 0.000004 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.4313 (1.0807) Loss: 1.4313 (1.0807)
2023-04-12,17:00:44 | INFO | Train Epoch: 1 [25206/29999 (84%)] Data (t): 0.001 Batch (t): 0.255, 23.6317/s, 23.6317/s/gpu LR: 0.000004 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.93961 (1.0774) Loss: 0.93961 (1.0774)
2023-04-12,17:01:09 | INFO | Train Epoch: 1 [25806/29999 (86%)] Data (t): 0.001 Batch (t): 0.255, 23.5839/s, 23.5839/s/gpu LR: 0.000004 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.3712 (1.0841) Loss: 1.3712 (1.0841)
2023-04-12,17:01:35 | INFO | Train Epoch: 1 [26406/29999 (88%)] Data (t): 0.001 Batch (t): 0.255, 23.4893/s, 23.4893/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.1438 (1.0854) Loss: 1.1438 (1.0854)
2023-04-12,17:02:00 | INFO | Train Epoch: 1 [27006/29999 (90%)] Data (t): 0.001 Batch (t): 0.255, 23.5418/s, 23.5418/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.77282 (1.0786) Loss: 0.77282 (1.0786)
2023-04-12,17:02:26 | INFO | Train Epoch: 1 [27606/29999 (92%)] Data (t): 0.001 Batch (t): 0.255, 23.5895/s, 23.5895/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.4402 (1.0863) Loss: 1.4402 (1.0863)
2023-04-12,17:02:51 | INFO | Train Epoch: 1 [28206/29999 (94%)] Data (t): 0.001 Batch (t): 0.255, 23.6328/s, 23.6328/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.4587 (1.0941) Loss: 1.4587 (1.0941)
2023-04-12,17:03:17 | INFO | Train Epoch: 1 [28806/29999 (96%)] Data (t): 0.001 Batch (t): 0.254, 23.6235/s, 23.6235/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.99690 (1.0921) Loss: 0.99690 (1.0921)
2023-04-12,17:03:42 | INFO | Train Epoch: 1 [29406/29999 (98%)] Data (t): 0.001 Batch (t): 0.255, 23.5799/s, 23.5799/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.34209 (1.0771) Loss: 0.34209 (1.0771)
2023-04-12,17:04:07 | INFO | Train Epoch: 1 [29994/29999 (100%)] Data (t): 0.001 Batch (t): 0.255, 23.6284/s, 23.6284/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.84085 (1.0725) Loss: 0.84085 (1.0725)
2023-04-12,17:04:12 | INFO | Start epoch 2
2023-04-12,17:04:13 | INFO | Train Epoch: 2 [    6/29999 (0%)] Data (t): 0.155 Batch (t): 0.369, 16.2788/s, 16.2788/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.79102 (0.79102) Loss: 0.79102 (0.79102)
2023-04-12,17:04:38 | INFO | Train Epoch: 2 [  606/29999 (2%)] Data (t): 0.001 Batch (t): 0.255, 23.6496/s, 23.6496/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.88315 (0.83708) Loss: 0.88315 (0.83708)
2023-04-12,17:05:04 | INFO | Train Epoch: 2 [ 1206/29999 (4%)] Data (t): 0.001 Batch (t): 0.255, 23.5081/s, 23.5081/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.1013 (0.92515) Loss: 1.1013 (0.92515)
2023-04-12,17:05:29 | INFO | Train Epoch: 2 [ 1806/29999 (6%)] Data (t): 0.001 Batch (t): 0.255, 23.5690/s, 23.5690/s/gpu LR: 0.000003 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.23609 (0.75289) Loss: 0.23609 (0.75289)
2023-04-12,17:05:55 | INFO | Train Epoch: 2 [ 2406/29999 (8%)] Data (t): 0.001 Batch (t): 0.255, 23.5646/s, 23.5646/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.90381 (0.78307) Loss: 0.90381 (0.78307)
2023-04-12,17:06:20 | INFO | Train Epoch: 2 [ 3006/29999 (10%)] Data (t): 0.001 Batch (t): 0.255, 23.6394/s, 23.6394/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.55783 (0.74553) Loss: 0.55783 (0.74553)
2023-04-12,17:06:46 | INFO | Train Epoch: 2 [ 3606/29999 (12%)] Data (t): 0.001 Batch (t): 0.255, 23.5808/s, 23.5808/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.51315 (0.71234) Loss: 0.51315 (0.71234)
2023-04-12,17:07:11 | INFO | Train Epoch: 2 [ 4206/29999 (14%)] Data (t): 0.001 Batch (t): 0.255, 23.5561/s, 23.5561/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.58386 (0.69628) Loss: 0.58386 (0.69628)
2023-04-12,17:07:37 | INFO | Train Epoch: 2 [ 4806/29999 (16%)] Data (t): 0.001 Batch (t): 0.255, 23.5737/s, 23.5737/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.94887 (0.72434) Loss: 0.94887 (0.72434)
2023-04-12,17:08:02 | INFO | Train Epoch: 2 [ 5406/29999 (18%)] Data (t): 0.001 Batch (t): 0.254, 23.5559/s, 23.5559/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.61445 (0.71335) Loss: 0.61445 (0.71335)
2023-04-12,17:08:28 | INFO | Train Epoch: 2 [ 6006/29999 (20%)] Data (t): 0.001 Batch (t): 0.256, 23.6634/s, 23.6634/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.52939 (0.69663) Loss: 0.52939 (0.69663)
2023-04-12,17:08:53 | INFO | Train Epoch: 2 [ 6606/29999 (22%)] Data (t): 0.001 Batch (t): 0.255, 23.5417/s, 23.5417/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.48183 (0.67873) Loss: 0.48183 (0.67873)
2023-04-12,17:09:19 | INFO | Train Epoch: 2 [ 7206/29999 (24%)] Data (t): 0.001 Batch (t): 0.255, 23.6100/s, 23.6100/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.99573 (0.70311) Loss: 0.99573 (0.70311)
2023-04-12,17:09:44 | INFO | Train Epoch: 2 [ 7806/29999 (26%)] Data (t): 0.001 Batch (t): 0.255, 23.4485/s, 23.4485/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.69181 (0.70231) Loss: 0.69181 (0.70231)
2023-04-12,17:10:10 | INFO | Train Epoch: 2 [ 8406/29999 (28%)] Data (t): 0.001 Batch (t): 0.256, 23.5530/s, 23.5530/s/gpu LR: 0.000002 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.36914 (0.68010) Loss: 0.36914 (0.68010)
2023-04-12,17:10:35 | INFO | Train Epoch: 2 [ 9006/29999 (30%)] Data (t): 0.001 Batch (t): 0.255, 23.6340/s, 23.6340/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.0947 (0.70601) Loss: 1.0947 (0.70601)
2023-04-12,17:11:01 | INFO | Train Epoch: 2 [ 9606/29999 (32%)] Data (t): 0.001 Batch (t): 0.255, 23.5730/s, 23.5730/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.42909 (0.68972) Loss: 0.42909 (0.68972)
2023-04-12,17:11:26 | INFO | Train Epoch: 2 [10206/29999 (34%)] Data (t): 0.001 Batch (t): 0.255, 23.6076/s, 23.6076/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.6579 (0.74351) Loss: 1.6579 (0.74351)
2023-04-12,17:11:52 | INFO | Train Epoch: 2 [10806/29999 (36%)] Data (t): 0.001 Batch (t): 0.255, 23.6561/s, 23.6561/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.1436 (0.76457) Loss: 1.1436 (0.76457)
2023-04-12,17:12:17 | INFO | Train Epoch: 2 [11406/29999 (38%)] Data (t): 0.001 Batch (t): 0.255, 23.5692/s, 23.5692/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.85744 (0.76921) Loss: 0.85744 (0.76921)
2023-04-12,17:12:42 | INFO | Train Epoch: 2 [12006/29999 (40%)] Data (t): 0.001 Batch (t): 0.255, 23.5909/s, 23.5909/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.52007 (0.75735) Loss: 0.52007 (0.75735)
2023-04-12,17:13:08 | INFO | Train Epoch: 2 [12606/29999 (42%)] Data (t): 0.001 Batch (t): 0.254, 23.4563/s, 23.4563/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.80214 (0.75938) Loss: 0.80214 (0.75938)
2023-04-12,17:13:33 | INFO | Train Epoch: 2 [13206/29999 (44%)] Data (t): 0.001 Batch (t): 0.255, 23.6147/s, 23.6147/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.74513 (0.75876) Loss: 0.74513 (0.75876)
2023-04-12,17:13:59 | INFO | Train Epoch: 2 [13806/29999 (46%)] Data (t): 0.001 Batch (t): 0.255, 23.4572/s, 23.4572/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.81089 (0.76093) Loss: 0.81089 (0.76093)
2023-04-12,17:14:24 | INFO | Train Epoch: 2 [14406/29999 (48%)] Data (t): 0.001 Batch (t): 0.255, 23.6338/s, 23.6338/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.84024 (0.76411) Loss: 0.84024 (0.76411)
2023-04-12,17:14:50 | INFO | Train Epoch: 2 [15006/29999 (50%)] Data (t): 0.001 Batch (t): 0.255, 23.4670/s, 23.4670/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.28773 (0.74578) Loss: 0.28773 (0.74578)
2023-04-12,17:15:15 | INFO | Train Epoch: 2 [15606/29999 (52%)] Data (t): 0.001 Batch (t): 0.255, 23.5817/s, 23.5817/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.98046 (0.75448) Loss: 0.98046 (0.75448)
2023-04-12,17:15:41 | INFO | Train Epoch: 2 [16206/29999 (54%)] Data (t): 0.001 Batch (t): 0.255, 23.4609/s, 23.4609/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.1647 (0.76913) Loss: 1.1647 (0.76913)
2023-04-12,17:16:06 | INFO | Train Epoch: 2 [16806/29999 (56%)] Data (t): 0.001 Batch (t): 0.254, 23.5989/s, 23.5989/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.12184 (0.74681) Loss: 0.12184 (0.74681)
2023-04-12,17:16:32 | INFO | Train Epoch: 2 [17406/29999 (58%)] Data (t): 0.001 Batch (t): 0.255, 23.6012/s, 23.6012/s/gpu LR: 0.000001 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.91169 (0.75230) Loss: 0.91169 (0.75230)
2023-04-12,17:16:57 | INFO | Train Epoch: 2 [18006/29999 (60%)] Data (t): 0.001 Batch (t): 0.254, 23.5825/s, 23.5825/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.78858 (0.75347) Loss: 0.78858 (0.75347)
2023-04-12,17:17:23 | INFO | Train Epoch: 2 [18606/29999 (62%)] Data (t): 0.001 Batch (t): 0.255, 23.5865/s, 23.5865/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.92759 (0.75891) Loss: 0.92759 (0.75891)
2023-04-12,17:17:48 | INFO | Train Epoch: 2 [19206/29999 (64%)] Data (t): 0.001 Batch (t): 0.255, 23.5732/s, 23.5732/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.62949 (0.75499) Loss: 0.62949 (0.75499)
2023-04-12,17:18:14 | INFO | Train Epoch: 2 [19806/29999 (66%)] Data (t): 0.001 Batch (t): 0.255, 23.6307/s, 23.6307/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.44767 (0.74595) Loss: 0.44767 (0.74595)
2023-04-12,17:18:39 | INFO | Train Epoch: 2 [20406/29999 (68%)] Data (t): 0.001 Batch (t): 0.256, 23.5730/s, 23.5730/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.0650 (0.75507) Loss: 1.0650 (0.75507)
2023-04-12,17:19:05 | INFO | Train Epoch: 2 [21006/29999 (70%)] Data (t): 0.001 Batch (t): 0.255, 23.4489/s, 23.4489/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.1831 (0.76696) Loss: 1.1831 (0.76696)
2023-04-12,17:19:30 | INFO | Train Epoch: 2 [21606/29999 (72%)] Data (t): 0.001 Batch (t): 0.255, 23.5888/s, 23.5888/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.24182 (0.75277) Loss: 0.24182 (0.75277)
2023-04-12,17:19:56 | INFO | Train Epoch: 2 [22206/29999 (74%)] Data (t): 0.001 Batch (t): 0.255, 23.5646/s, 23.5646/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.0718 (0.76116) Loss: 1.0718 (0.76116)
2023-04-12,17:20:21 | INFO | Train Epoch: 2 [22806/29999 (76%)] Data (t): 0.001 Batch (t): 0.255, 23.5654/s, 23.5654/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.6624 (0.78427) Loss: 1.6624 (0.78427)
2023-04-12,17:20:47 | INFO | Train Epoch: 2 [23406/29999 (78%)] Data (t): 0.001 Batch (t): 0.255, 23.5742/s, 23.5742/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.93420 (0.78802) Loss: 0.93420 (0.78802)
2023-04-12,17:21:12 | INFO | Train Epoch: 2 [24006/29999 (80%)] Data (t): 0.001 Batch (t): 0.255, 23.4930/s, 23.4930/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.91181 (0.79104) Loss: 0.91181 (0.79104)
2023-04-12,17:21:38 | INFO | Train Epoch: 2 [24606/29999 (82%)] Data (t): 0.001 Batch (t): 0.255, 23.5397/s, 23.5397/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.0845 (0.79803) Loss: 1.0845 (0.79803)
2023-04-12,17:22:03 | INFO | Train Epoch: 2 [25206/29999 (84%)] Data (t): 0.001 Batch (t): 0.255, 23.4416/s, 23.4416/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.66087 (0.79484) Loss: 0.66087 (0.79484)
2023-04-12,17:22:29 | INFO | Train Epoch: 2 [25806/29999 (86%)] Data (t): 0.001 Batch (t): 0.255, 23.5677/s, 23.5677/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.67262 (0.79206) Loss: 0.67262 (0.79206)
2023-04-12,17:22:54 | INFO | Train Epoch: 2 [26406/29999 (88%)] Data (t): 0.001 Batch (t): 0.255, 23.6009/s, 23.6009/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.43051 (0.78402) Loss: 0.43051 (0.78402)
2023-04-12,17:23:20 | INFO | Train Epoch: 2 [27006/29999 (90%)] Data (t): 0.001 Batch (t): 0.255, 23.5939/s, 23.5939/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.42141 (0.77614) Loss: 0.42141 (0.77614)
2023-04-12,17:23:45 | INFO | Train Epoch: 2 [27606/29999 (92%)] Data (t): 0.001 Batch (t): 0.256, 23.4193/s, 23.4193/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.67847 (0.77406) Loss: 0.67847 (0.77406)
2023-04-12,17:24:11 | INFO | Train Epoch: 2 [28206/29999 (94%)] Data (t): 0.001 Batch (t): 0.255, 23.5755/s, 23.5755/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 1.2730 (0.78446) Loss: 1.2730 (0.78446)
2023-04-12,17:24:36 | INFO | Train Epoch: 2 [28806/29999 (96%)] Data (t): 0.001 Batch (t): 0.255, 23.5544/s, 23.5544/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.62536 (0.78121) Loss: 0.62536 (0.78121)
2023-04-12,17:25:02 | INFO | Train Epoch: 2 [29406/29999 (98%)] Data (t): 0.001 Batch (t): 0.255, 23.5799/s, 23.5799/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.72775 (0.78014) Loss: 0.72775 (0.78014)
2023-04-12,17:25:27 | INFO | Train Epoch: 2 [29994/29999 (100%)] Data (t): 0.001 Batch (t): 0.255, 23.6727/s, 23.6727/s/gpu LR: 0.000000 Logit Scale: 100.000 Contrastive_loss: 0.0000 (0.0000) Caption_loss: 0.86041 (0.78172) Loss: 0.86041 (0.78172)
